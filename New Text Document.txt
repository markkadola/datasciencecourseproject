train_res, test_res, accuracy_res, f1_res, precision_res, recall_res = [], [], [], [], [], []

for train_idx, test_idx in kf.split(X1, y):
    X1_train, y_train = X1.iloc[train_idx], y[train_idx]
    X1_test, y_test = X1.iloc[test_idx], y[test_idx]

    mdl = SVC(
    C=1., 
    kernel='linear' 
).fit(X1_train, y_train)
    
    y_train_preds, y_test_preds = mdl.predict(X1_train), mdl.predict(X1_test)
    
    train_res.append(np.round(mean_squared_error(y_train, y_train_preds), 2))
    test_res.append(np.round(mean_squared_error(y_test, y_test_preds), 2))

    accuracy_res.append(np.round(accuracy_score(y_train, y_train_preds), 2))
    f1_res.append(np.round(f1_score(y_test, y_test_preds, average='weighted'), 2))
    precision_res.append(np.round(precision_score(y_train, y_train_preds, average='weighted'), 2))
    recall_res.append(np.round(recall_score(y_test, y_test_preds, average='weighted'), 2))

print('Accuracy: ', np.mean(accuracy_res))
print('F1: ', np.mean(f1_res))
print('Precision: ', np.mean(precision_res))
print('Recall: ', np.mean(recall_res))    

print(f'Очікувана помилка на навчальному наборі: {np.mean(train_res)} +- {np.std(train_res)}')
print(f'Очікувана помилка на тестовому наборі: {np.mean(test_res)} +- {np.std(test_res)}')


Розглядались набори даних, пов'язані з червоним і білим португальськими винами. 
Було проведено аналіз атрибутів і визначено їх тип.
Для визначення типу атрибутів визначалась кількість унікальних значень для кожного з них. Виявилося, що тільки "quality" є категорним. 
Дослідили зв'язки між параметрами та залежною змінною. 
Для цього обчислювались коефіцієнти кореляції. Виявилося, що чотири параметри (volatile acidity, chlorides, density, alcohol) мають значну залежність. На основі цих даних, обрано параметри, що увійшли до подальшого аналізу. 
Проведено попередню обробку даних (візуалізація даних, перевірка на пропущені значення, на нечислові значення і прологарифмовані дані)
У вхідному дата-фреймі пропущених і нечислових значень не виявлено.
Було побудовано графіки залежності кожного параметру від залежної змінної. Візуально було помітно, що alcohol і ph схожі на нормальний розподіл. Характеристикам density і fixed acidity більш за все притаманний рівномірний розподіл.
Прологарифмували всі параметри, крім density, citric і sulphates (тому що в них присутні нульові значення). Побудували графіки залежності кожного отриманого параметру від залежної змінної. Візуально всі графіки параметрів покращилися. 
На найпростіщій моделі логістичної регресії перевірили чи покращиться модель з прологарифмованими даними. Такі метрики, як auc, середня помилка на навчальному та тестовому наборі покращилися. Тому всі моделі будуватимемо на прологарифмованих даних.
Розглядались наступні моделі: логістична регресія, метод опорних векторів, к-середнє, к-найближчих сусідів, дерева рішень і випадковий ліс.
Для покращення моделей використовувались крос-валідація для навчання та оцінки моделей і підбір гіперпараметрів.
Для покращення моделі побудували логістичну регресію з адопомогою крос-валідації. Порівняно з логарифмованою регресією, метрики accuracy, precision, recall поліпшились. Метрики F1, auc, fault train, fault test погіршились, але вони усе одно краще, ніж відповідні метрики для непрологарифмованої логістичної регресії.
Автоматично здійснили підбір гіперпараметрів для методу логістичної регресії. Побудували модель з урахуванням запропонованих параметрів. Покращити модель вдалося, але поліпшення виявилися незначними.
Побудували просту модель без розбиття на тестовий та навчальний набори за методом опорних векторів. Для цього методу всі метрики виявились гіршими, ніж відповідні для логістичної регресії.
Побудували модель опорних векторів за допомогою крос-валідації. Модель дещо покращилась, порівняно без крос-валідації.
Автоматично здійснили підбір гіперпараметрів для методу опорних векторів. Побудували модель з урахуванням запропонованих параметрів. Після перевірки модель незначно погіршилась. 
Спробували вручну інші параметри. Покращили модель опорних векторів. Отримали показники приблизно такі ж самі, як і для логістичної регресії.
Побудували просту модель методом k-середніх. Графічно виявили оптимальну кілкість кластерів.
Здійснили підбір гіперпараметрів для методу k-середніх. Побудували модель за допомогою крос-валідації і з урахуванням запропонованих параметрів. Отримали дуже погані результати. 
Побудували модель k-найближчих сусідів за допомогою крос-валідації. Отримали результат приблизно такий же, як і для логістичної регресії (краще лише параметр f1 для моделі найближчих сусідів). Параметри f1 і precision краще, ніж для моделі SVC.
Автоматично знійснили підбір гіперпараметрів для методу k-найближчих сусідів. Результат покращився не тільки порівняно із моделю KNeighbors без підбору гіперпараметрів, але також був найкращим серед усіх моделей.
Побудували модель дерева рішень за допомогою крос-валідації. Тому що ми узяли максимальну глибину тільки 2, результати вийшли погані. 
Автоматично здійснили підбір гіперпараметрів для дерев рішень. Результати помітно покращились, але отримали досі незадовільний результат.
Спробували підібрати параметри вручну. Результати метрик покращились відносно моделі з автоматично підібраними параметрами, але вона не стала найкращою.
Побудували модель випадкового лісу. Отримали найкращий результат серед усіх попередніх моделей, але він був ще не надто хороший.
Побудували модель випадковго лісу за допомогою крос-валідації. За параметрами f1 і auc модель незначно покращилася.
Автоматично здійснили підбір гіперпараметрів для випадкового лісу. Модель покращилась, але результати все ще були незадовільні.
Підібрали деякі параметри за допомогою яких результати значно покращились. Отримали найкращий результат з усіх попередніх моделей.
Найкраща модель виявилась моделю випадкового лісу. 
